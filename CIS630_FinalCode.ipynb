{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee24a1e-f512-4b3f-93b2-84e24e0f61ff",
   "metadata": {},
   "source": [
    "Installing imports and setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d4d1d-5356-4c69-82cb-37f999c2296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets pillow matplotlib --quiet\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# setting parameters\n",
    "IMG_WIDTH, IMG_HEIGHT = 128, 32\n",
    "MAX_LABEL_LENGTH = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb16d5d-b539-469e-828d-46c5f6b12049",
   "metadata": {},
   "source": [
    "Preproccessing the images and merging into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1ccd8-5433-49a5-8d29-0db78aefc9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loading in the dataset IAM\n",
    "dataset = load_dataset(\"Teklia/IAM-line\")\n",
    "sample = dataset['train'][0]\n",
    "print(\"IAM Sample text:\", sample['text'])\n",
    "sample['image'].show()\n",
    "\n",
    "# function to preprocess IAM\n",
    "def preprocess_image_pil(image):\n",
    "    image = image.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    image = np.array(image.convert(\"L\")) / 255.0  \n",
    "    return np.expand_dims(image, axis=-1)\n",
    "\n",
    "iam_texts = [ex['text'] for ex in dataset['train']]\n",
    "\n",
    "# the tokenizer for IAM\n",
    "# Note: Tokenizer assigns tokens starting at 1, and 0 will be used for padding.\n",
    "tokenizer = Tokenizer(char_level=True, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(iam_texts)\n",
    "\n",
    "M = max(tokenizer.word_index.values())\n",
    "# We remap valid tokens to range 0 ... M-1. Then we reserve index M as the blank.\n",
    "NUM_CLASSES = M + 1\n",
    "print(\"Computed NUM_CLASSES (including blank classes):\", NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8922685-2c64-4068-bc98-4b5d1de096a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing the images and labels for IAM dataset\n",
    "train_images = np.array([preprocess_image_pil(ex['image']) for ex in dataset['train']])\n",
    "\n",
    "# Get label sequences (values are in [1, M])\n",
    "train_labels = tokenizer.texts_to_sequences(iam_texts)\n",
    "\n",
    "# Pad with 0 (which is also our pad value)\n",
    "train_labels = pad_sequences(train_labels, maxlen=MAX_LABEL_LENGTH, padding='post', value=0)\n",
    "\n",
    "# Remap: subtract 1 from nonzero entries so that valid tokens become 0..M-1.\n",
    "# (Padded zeros remain 0 â€“ later we compute label lengths by counting nonzeros.)\n",
    "train_labels = np.where(train_labels != 0, train_labels - 1, 0)\n",
    "\n",
    "print(\"IAM images shape:\", train_images.shape)\n",
    "print(\"IAM labels shape:\", train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b033b05-0cfa-4269-9085-892d741dcbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples_image_folder = \"jpg\"  \n",
    "samples_label_file = \"/Users/shikhusanjel/Desktop/CNNTEST/SamplePassage.txt\"\n",
    "\n",
    "# reading the labels line by line\n",
    "with open(samples_label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# list and sort collected data image paths \n",
    "image_paths = sorted([\n",
    "    os.path.join(samples_image_folder, f)\n",
    "    for f in os.listdir(samples_image_folder)\n",
    "    if f.lower().endswith(\".jpg\")\n",
    "])\n",
    "print(\"Custom image count:\", len(image_paths))\n",
    "print(\"Custom label count:\", len(custom_labels))\n",
    "\n",
    "def preprocess_image_from_path(path):\n",
    "    img = load_img(path, color_mode='grayscale', target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    img = img_to_array(img) / 255.0\n",
    "    return img\n",
    "\n",
    "custom_images_processed = np.array([preprocess_image_from_path(p) for p in image_paths])\n",
    "custom_sequences = tokenizer.texts_to_sequences(custom_labels)\n",
    "custom_sequences_padded = pad_sequences(custom_sequences, maxlen=MAX_LABEL_LENGTH, padding='post', value=0)\n",
    "custom_sequences_padded = np.where(custom_sequences_padded != 0, custom_sequences_padded - 1, 0)\n",
    "\n",
    "print(\"Custom images shape:\", custom_images_processed.shape)\n",
    "print(\"Custom labels shape:\", custom_sequences_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74311ef7-b40f-4de7-8899-e0180c6de4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the IAM and the custom data\n",
    "merged_images = np.concatenate([train_images, custom_images_processed], axis=0)\n",
    "merged_labels = np.concatenate([train_labels, custom_sequences_padded], axis=0)\n",
    "print(\"Merged images shape:\", merged_images.shape)\n",
    "print(\"Merged labels shape:\", merged_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57827a-9d8c-4685-bbfc-67e28f2d9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    merged_images, merged_labels, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3536c-7938-42ae-9172-a586d7682c49",
   "metadata": {},
   "source": [
    "Building the model CNN and CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866a1a5-93e1-4c16-99b9-18911e81428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first run through of training merged data CELL BLOCK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917a87f-f293-4fd5-a2b8-5296642f2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model inputs\n",
    "input_img = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1), name='image_input')\n",
    "labels_input = layers.Input(name='label', shape=(MAX_LABEL_LENGTH,), dtype='int32')\n",
    "\n",
    "# CNN + BiLSTM Architecture\n",
    "x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Reshape((-1, 128))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "x = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4b1cd-603f-42fb-bc7f-e2a41f408b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC loss using a lambda layer\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels = args\n",
    "    # Prediction lengths: each sample has the same time dimension.\n",
    "    time_steps = tf.cast(tf.shape(y_pred)[1], tf.float32)\n",
    "    input_length = tf.ones((tf.shape(y_pred)[0], 1), dtype=tf.float32) * time_steps\n",
    "    # Actual label length: count nonzero (i.e. valid) labels per sample.\n",
    "    label_length = tf.cast(tf.math.count_nonzero(labels, axis=1, keepdims=True), dtype=tf.float32)\n",
    "    return tf.keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc_loss')([x, labels_input])\n",
    "\n",
    "# final model\n",
    "ctc_model = Model(inputs=[input_img, labels_input], outputs=loss_out, name=\"CTC_Model\")\n",
    "ctc_model.compile(optimizer='adam', loss=lambda y_true, y_pred: y_pred)\n",
    "\n",
    "dummy_train = np.zeros((len(X_train), 1))\n",
    "dummy_val = np.zeros((len(X_val), 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29cc68-0521-473a-9ae3-eef267f8bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "history = ctc_model.fit(\n",
    "    [X_train, y_train],\n",
    "    dummy_train,\n",
    "    validation_data=([X_val, y_val], dummy_val),\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7523a1e8-416b-4fe3-9bae-12d91a92e546",
   "metadata": {},
   "source": [
    "Creating the Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866aaedc-4dd3-4191-b4a7-2a68a348776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a separate model that maps the image input to predictions (the softmax outputs)\n",
    "\n",
    "# Get the image input from our original model\n",
    "# (Note: our original model 'ctc_model' uses two inputs, but the prediction is based solely on the image input.)\n",
    "prediction_model = Model(inputs=input_img, outputs=x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed751adf-acec-43f6-a586-d5eb4c9467b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# decode predictions using TensorFlow their built in CTC decoder\n",
    "def decode_predictions(preds, charset):\n",
    "    # preds: output from the prediction model, shape (batch, time_steps, NUM_CLASSES)\n",
    "    # The 'input_length' for each sample is the full length of the time dimension:\n",
    "    input_length = np.ones(preds.shape[0]) * preds.shape[1]\n",
    "    # Use greedy CTC decode:\n",
    "    decoded, log_prob = tf.keras.backend.ctc_decode(preds, input_length, greedy=True)\n",
    "    decoded_sequences = decoded[0].numpy()\n",
    "    results = []\n",
    "    for seq in decoded_sequences:\n",
    "        # Remove any padding or repeated characters:\n",
    "        # Since we remapped token values earlier, our tokens are in [0, M-1]. To convert back:\n",
    "        result = \"\"\n",
    "        for token in seq:\n",
    "            if token == -1:\n",
    "                continue\n",
    "            # Token value 0 corresponds to the lowest valid character.\n",
    "            # Our tokenizer originally had word_index starting at 1,\n",
    "            # and we subtracted one; so we add 1 back to look up the character.\n",
    "            char = charset.get(token + 1, \"\")\n",
    "            result += char\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# build a reverse mapping dictionary from token index to character \n",
    "#(this is in order to read the test images and decode their outputs to evalluate them\n",
    "reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "index_to_char = {index: char for char, index in tokenizer.word_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef6cd9d-6414-4bbb-9216-447f7342a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 5\n",
    "sample_imgs = X_val[:num_samples]\n",
    "sample_labels = y_val[:num_samples]\n",
    "\n",
    "#get the predictions\n",
    "preds = prediction_model.predict(sample_imgs)\n",
    "\n",
    "#decoding the predictions \n",
    "decoded_texts = []\n",
    "for pred in preds:\n",
    "    # CTC decode the prediction for a single sample\n",
    "    input_len = np.array([pred.shape[0]])\n",
    "    # The ctc_decode expects batch shape; we add batch dimension.\n",
    "    decoded, _ = tf.keras.backend.ctc_decode(np.expand_dims(pred, axis=0), input_length=input_len, greedy=True)\n",
    "    decoded_seq = decoded[0].numpy()[0]\n",
    "    text = \"\"\n",
    "    for token in decoded_seq:\n",
    "        # Skip blank tokens (if any); token values range from 0 to (M-1)\n",
    "        if token < 0: \n",
    "            continue\n",
    "        text += index_to_char.get(token + 1, \"\")\n",
    "    decoded_texts.append(text)\n",
    "\n",
    "# predictions compared to the ground truth\n",
    "for i in range(num_samples):\n",
    "    plt.figure(figsize=(6, 2))\n",
    "    plt.imshow(sample_imgs[i].squeeze(), cmap='gray')\n",
    "    plt.title(f\"Predicted: {decoded_texts[i]}\\nGround Truth: \" +\n",
    "              \"\".join([index_to_char.get(token+1, \"\") for token in sample_labels[i] if token > 0]))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e6286b-3b77-4834-9781-35fd1d4027a0",
   "metadata": {},
   "source": [
    "This is Training and Testing of the first attempt before adding in more epochs and etc. the one above is the most recent model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f0470-ea48-407c-983f-1f0a68774d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets pillow matplotlib --quiet\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# setting parameters\n",
    "IMG_WIDTH, IMG_HEIGHT = 128, 32\n",
    "MAX_LABEL_LENGTH = 110\n",
    "\n",
    "\n",
    "\n",
    "# --- Load IAM line-level handwriting data ---\n",
    "dataset = load_dataset(\"Teklia/IAM-line\")\n",
    "sample = dataset['train'][0]\n",
    "print(\"IAM Sample text:\", sample['text'])\n",
    "sample['image'].show()\n",
    "\n",
    "# --- Preprocessing function for IAM images (PIL based) ---\n",
    "def preprocess_image_pil(image):\n",
    "    image = image.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    image = np.array(image.convert(\"L\")) / 255.0  \n",
    "    return np.expand_dims(image, axis=-1)\n",
    "\n",
    "# Process IAM dataset texts and images\n",
    "iam_texts = [ex['text'] for ex in dataset['train']]\n",
    "\n",
    "# Build a tokenizer on IAM texts (character-level)\n",
    "# Note: Tokenizer assigns tokens starting at 1, and 0 will be used for padding.\n",
    "tokenizer = Tokenizer(char_level=True, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(iam_texts)\n",
    "\n",
    "# Compute NUM_CLASSES automatically.\n",
    "# Let M = maximum token assigned by the tokenizer.\n",
    "M = max(tokenizer.word_index.values())\n",
    "# We remap valid tokens to range 0 ... M-1. Then we reserve index M as the blank.\n",
    "NUM_CLASSES = M + 1\n",
    "print(\"Computed NUM_CLASSES (including blank):\", NUM_CLASSES)\n",
    "\n",
    "# Process images and labels for IAM data\n",
    "train_images = np.array([preprocess_image_pil(ex['image']) for ex in dataset['train']])\n",
    "# Get label sequences (values are in [1, M])\n",
    "train_labels = tokenizer.texts_to_sequences(iam_texts)\n",
    "# Pad with 0 (which is also our pad value)\n",
    "train_labels = pad_sequences(train_labels, maxlen=MAX_LABEL_LENGTH, padding='post', value=0)\n",
    "# Remap: subtract 1 from nonzero entries so that valid tokens become 0..M-1.\n",
    "# (Padded zeros remain 0 â€“ later we compute label lengths by counting nonzeros.)\n",
    "train_labels = np.where(train_labels != 0, train_labels - 1, 0)\n",
    "\n",
    "print(\"IAM images shape:\", train_images.shape)\n",
    "print(\"IAM labels shape:\", train_labels.shape)\n",
    "\n",
    "# --- Process Custom Dataset ---\n",
    "# Use your original paths\n",
    "samples_image_folder = \"jpg\"  \n",
    "samples_label_file = \"/Users/shikhusanjel/Desktop/CNNTEST/SamplePassage.txt\"\n",
    "\n",
    "# Read the custom labels (each line corresponds to one image)\n",
    "with open(samples_label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# List and sort custom image paths (order should align with custom_labels)\n",
    "image_paths = sorted([\n",
    "    os.path.join(samples_image_folder, f)\n",
    "    for f in os.listdir(samples_image_folder)\n",
    "    if f.lower().endswith(\".jpg\")\n",
    "])\n",
    "print(\"Custom image count:\", len(image_paths))\n",
    "print(\"Custom label count:\", len(custom_labels))\n",
    "\n",
    "def preprocess_image_from_path(path):\n",
    "    img = load_img(path, color_mode='grayscale', target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    img = img_to_array(img) / 255.0\n",
    "    return img\n",
    "\n",
    "custom_images_processed = np.array([preprocess_image_from_path(p) for p in image_paths])\n",
    "custom_sequences = tokenizer.texts_to_sequences(custom_labels)\n",
    "custom_sequences_padded = pad_sequences(custom_sequences, maxlen=MAX_LABEL_LENGTH, padding='post', value=0)\n",
    "custom_sequences_padded = np.where(custom_sequences_padded != 0, custom_sequences_padded - 1, 0)\n",
    "\n",
    "print(\"Custom images shape:\", custom_images_processed.shape)\n",
    "print(\"Custom labels shape:\", custom_sequences_padded.shape)\n",
    "\n",
    "# --- Merge the IAM and Custom datasets ---\n",
    "merged_images = np.concatenate([train_images, custom_images_processed], axis=0)\n",
    "merged_labels = np.concatenate([train_labels, custom_sequences_padded], axis=0)\n",
    "print(\"Merged images shape:\", merged_images.shape)\n",
    "print(\"Merged labels shape:\", merged_labels.shape)\n",
    "\n",
    "# --- Split data into training and validation sets ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    merged_images, merged_labels, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "\n",
    "#############################################\n",
    "# Part 2: Build and Train the CRNN with CTC Loss\n",
    "#############################################\n",
    "\n",
    "# ---- Define the Model Inputs ----\n",
    "input_img = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1), name='image_input')\n",
    "labels_input = layers.Input(name='label', shape=(MAX_LABEL_LENGTH,), dtype='int32')\n",
    "\n",
    "# ---- Build CNN + BiLSTM Architecture ----\n",
    "x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Reshape((-1, 128))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "x = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "# ---- Define CTC Loss via a Lambda Layer ----\n",
    "# Here we compute each sample's label length by counting nonzero entries.\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels = args\n",
    "    # Prediction lengths: each sample has the same time dimension.\n",
    "    time_steps = tf.cast(tf.shape(y_pred)[1], tf.float32)\n",
    "    input_length = tf.ones((tf.shape(y_pred)[0], 1), dtype=tf.float32) * time_steps\n",
    "    # Actual label length: count nonzero (i.e. valid) labels per sample.\n",
    "    label_length = tf.cast(tf.math.count_nonzero(labels, axis=1, keepdims=True), dtype=tf.float32)\n",
    "    return tf.keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc_loss')([x, labels_input])\n",
    "\n",
    "# ---- Build the Final Model ----\n",
    "ctc_model = Model(inputs=[input_img, labels_input], outputs=loss_out, name=\"CTC_Model\")\n",
    "ctc_model.compile(optimizer='adam', loss=lambda y_true, y_pred: y_pred)\n",
    "\n",
    "# ---- Prepare Dummy Targets (since our model outputs the loss directly) ----\n",
    "dummy_train = np.zeros((len(X_train), 1))\n",
    "dummy_val = np.zeros((len(X_val), 1))\n",
    "\n",
    "# ---- Train the Model ----\n",
    "history = ctc_model.fit(\n",
    "    [X_train, y_train],\n",
    "    dummy_train,\n",
    "    validation_data=([X_val, y_val], dummy_val),\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Part 3: Create a Prediction Model\n",
    "# -------------------------------\n",
    "# Create a separate model that maps the image input to predictions (the softmax outputs)\n",
    "\n",
    "# Get the image input from our original model\n",
    "# (Note: our original model 'ctc_model' uses two inputs, but the prediction is based solely on the image input.)\n",
    "prediction_model = Model(inputs=input_img, outputs=x)\n",
    "\n",
    "# -------------------------------\n",
    "# Part 4: Decode Predictions and Evaluate\n",
    "# -------------------------------\n",
    "\n",
    "# Function to decode predictions using TensorFlow's built-in CTC decoder\n",
    "def decode_predictions(preds, charset):\n",
    "    # preds: output from the prediction model, shape (batch, time_steps, NUM_CLASSES)\n",
    "    # The 'input_length' for each sample is the full length of the time dimension:\n",
    "    input_length = np.ones(preds.shape[0]) * preds.shape[1]\n",
    "    # Use greedy CTC decode:\n",
    "    decoded, log_prob = tf.keras.backend.ctc_decode(preds, input_length, greedy=True)\n",
    "    decoded_sequences = decoded[0].numpy()\n",
    "    results = []\n",
    "    for seq in decoded_sequences:\n",
    "        # Remove any padding or repeated characters:\n",
    "        # Since we remapped token values earlier, our tokens are in [0, M-1]. To convert back:\n",
    "        result = \"\"\n",
    "        for token in seq:\n",
    "            if token == -1:\n",
    "                continue\n",
    "            # Token value 0 corresponds to the lowest valid character.\n",
    "            # Our tokenizer originally had word_index starting at 1,\n",
    "            # and we subtracted one; so we add 1 back to look up the character.\n",
    "            char = charset.get(token + 1, \"\")\n",
    "            result += char\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# Build a reverse mapping dictionary from token index to character\n",
    "reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "# Actually, we want index -> char, so:\n",
    "index_to_char = {index: char for char, index in tokenizer.word_index.items()}\n",
    "\n",
    "# For testing, select a few images from the validation set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 5\n",
    "sample_imgs = X_val[:num_samples]\n",
    "sample_labels = y_val[:num_samples]\n",
    "\n",
    "# Get model predictions\n",
    "preds = prediction_model.predict(sample_imgs)\n",
    "\n",
    "# Decode predictions; note: since we remapped token indices by subtracting 1,\n",
    "# valid tokens are in range [0, M-1] and to look up the corresponding character,\n",
    "# we add 1 back.\n",
    "decoded_texts = []\n",
    "for pred in preds:\n",
    "    # CTC decode the prediction for a single sample\n",
    "    input_len = np.array([pred.shape[0]])\n",
    "    # The ctc_decode expects batch shape; we add batch dimension.\n",
    "    decoded, _ = tf.keras.backend.ctc_decode(np.expand_dims(pred, axis=0), input_length=input_len, greedy=True)\n",
    "    decoded_seq = decoded[0].numpy()[0]\n",
    "    text = \"\"\n",
    "    for token in decoded_seq:\n",
    "        # Skip blank tokens (if any); token values range from 0 to (M-1)\n",
    "        if token < 0: \n",
    "            continue\n",
    "        text += index_to_char.get(token + 1, \"\")\n",
    "    decoded_texts.append(text)\n",
    "\n",
    "# Show the sample images with their decoded predictions and ground truth\n",
    "for i in range(num_samples):\n",
    "    plt.figure(figsize=(6, 2))\n",
    "    plt.imshow(sample_imgs[i].squeeze(), cmap='gray')\n",
    "    plt.title(f\"Predicted: {decoded_texts[i]}\\nGround Truth: \" +\n",
    "              \"\".join([index_to_char.get(token+1, \"\") for token in sample_labels[i] if token > 0]))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
